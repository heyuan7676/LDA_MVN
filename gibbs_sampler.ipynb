{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"Features.csv\",sep='\\t',index_col = 0)\n",
    "#data = data.iloc[np.random.choice(range(len(data)),5000)]\n",
    "words_in_each_gene = data.groupby(data.index).sum()\n",
    "\n",
    "gene_exp = pd.read_csv('../data/gene_expression/Whole_Blood__rsem_uniquelymapped_genes_certain_biotypes_TPM_withGeneNames.txt',sep='\\t', index_col = 0)\n",
    "gene_exp_distribution = np.log(np.mean(gene_exp + 1e-10, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495584"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_txt = [np.repeat(np.array(words_in_each_gene.columns), t.astype('int'),axis=0) for t in np.array(np.ceil(words_in_each_gene))]\n",
    "len([a for b in words_in_txt for a in b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baits_exp = [np.mean(np.mean(np.exp(gene_exp.loc[np.intersect1d(x.split('_'),np.array(gene_exp.index))]),axis=1))  for x in list(words_in_each_gene.index)]\n",
    "baits_exp = np.array(baits_exp)\n",
    "baits_exp[np.isnan(baits_exp)] = 1e-10\n",
    "baits_exp = np.log(baits_exp)\n",
    "\n",
    "corpos = (baits_exp < np.percentile(gene_exp_distribution, 75)) * 1\n",
    "corpos = [str(t) for t in corpos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_in_txt = [np.repeat(np.array(words_in_each_gene.columns), t.astype('int'),axis=0) for t in np.array(np.ceil(words_in_each_gene))]\n",
    "\n",
    "\n",
    "train_idx = np.random.choice(range(len(corpos)), int(len(corpos) * 0.8), replace=False)\n",
    "test_idx = [a for a in range(len(corpos)) if a not in train_idx]\n",
    "\n",
    "output = open('gibbs/input-train-median.txt','w')\n",
    "for k in list(train_idx):\n",
    "    output.write('\\t'.join([corpos[k]] + list(words_in_txt[k])))\n",
    "    output.write('\\n')\n",
    "output.close()\n",
    "\n",
    "\n",
    "\n",
    "output = open('gibbs/input-test-median.txt','w')\n",
    "for k in list(test_idx):\n",
    "    output.write('\\t'.join([corpos[k]] + list(words_in_txt[k])))\n",
    "    output.write('\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 13)\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from numpy import random\n",
    "import time\n",
    "import bisect\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "class literature:\n",
    "    def __init__(self, filename_train, filename_test, K):\n",
    "        ## all topics\n",
    "        self.K = K\n",
    "        ## all documents: training + test\n",
    "        self.documents_train, self.documents_test = list(), list()\n",
    "        ## train\n",
    "        datafile = open(filename_train)\n",
    "        for l in datafile:\n",
    "            self.documents_train.append(document(l,K))\n",
    "        ## test\n",
    "        datafile = open(filename_test)\n",
    "        for l in datafile:\n",
    "            self.documents_test.append(document(l,K))\n",
    "        self.D = len(self.documents_train) + len(self.documents_test)\n",
    "        ## all words\n",
    "        all_words = list(chain(*[d.words for d in (self.documents_train + self.documents_test)]))\n",
    "        all_words = list(set(all_words))\n",
    "        ## length of all words\n",
    "        V = len(all_words)\n",
    "        self.V = V\n",
    "        ## words index in all documents\n",
    "        self.words_index = dict(zip(all_words, range(V)))\n",
    "        ## fill in index for all documents\n",
    "        for d in self.documents_train:\n",
    "            d.word_index(self.words_index)\n",
    "        for d in self.documents_test:\n",
    "            d.word_index(self.words_index)\n",
    "        ## initialize Nwk for training data\n",
    "        self.cal_Nwk()\n",
    "\n",
    "    def cal_Nwk(self):\n",
    "        ## Nwk\n",
    "        V = self.V\n",
    "        K = self.K\n",
    "        self.Nwk, self.Nwk_0, self.Nwk_1 = np.zeros([V,K]), np.zeros([V,K]), np.zeros([V,K])     ### V x K\n",
    "        ## Nwk(s): V x K. ordered lists of ordered lists\n",
    "        for d in self.documents_train:\n",
    "            d.cal_nwk(self.V, self.K)\n",
    "            self.Nwk = self.Nwk + d.nwk\n",
    "            self.Nwk_0 = self.Nwk_0 + d.nwk_0\n",
    "            self.Nwk_1 = self.Nwk_1 + d.nwk_1\n",
    "        self.Nk = np.sum(self.Nwk, axis = 0)\n",
    "        self.Nk_0 = np.sum(self.Nwk_0, axis = 0)\n",
    "        self.Nk_1 = np.sum(self.Nwk_1, axis = 0)\n",
    "    \n",
    "    def gibbs_sampler(self, datatype):\n",
    "        if datatype == 'train':\n",
    "            for d in self.documents_train:\n",
    "                d.update_z_x_train(self.Nk, self.Nk_0, self.Nk_1, self.Nwk, self.Nwk_0, self.Nwk_1, self.V, self.K)\n",
    "        elif datatype == 'test':\n",
    "            for d in self.documents_test:\n",
    "                d.update_z_x_test(self.Nk, self.Nk_0, self.Nk_1, self.Nwk, self.Nwk_0, self.Nwk_1, self.V, self.K)\n",
    "        \n",
    "    def MAP_estimate(self, datatype):\n",
    "        if datatype == 'train':\n",
    "            for d in self.documents_train:\n",
    "                d.estimate_theta(self.K)\n",
    "            ### equation 6 - 7\n",
    "            self.phi   = np.divide((self.Nwk + beta), (self.Nk + self.V * beta))     ### V x K\n",
    "            self.phi_0 = np.divide((self.Nwk_0 + beta), (self.Nk_0 + self.V * beta))\n",
    "            self.phi_1 = np.divide((self.Nwk_1 + beta), (self.Nk_1 + self.V * beta))\n",
    "            self.phi_sum = np.sum(self.phi, axis = 0)\n",
    "            self.phi_0_sum = np.sum(self.phi_0, axis = 0)\n",
    "            self.phi_1_sum = np.sum(self.phi_1, axis = 0)\n",
    "        elif datatype == 'test':\n",
    "            ### theta for each document\n",
    "            for d in self.documents_test:\n",
    "                d.estimate_theta(self.K)\n",
    "\n",
    "    def cal_llk(self, datatype):\n",
    "        ### equation 8\n",
    "        llk = 0.0\n",
    "        if datatype == 'train':\n",
    "            documents = self.documents_train\n",
    "        else:\n",
    "            documents = self.documents_test\n",
    "        for d in documents:\n",
    "            each_d = 0.0\n",
    "            if d.corpus == 1:\n",
    "                phi_c = self.phi_1\n",
    "            else:\n",
    "                phi_c = self.phi_0\n",
    "            for w in d.words:\n",
    "                current_v = w\n",
    "                most_inner = (1-lmda)*self.phi[current_v,] + lmda*(phi_c[current_v,])\n",
    "                unit = np.multiply(d.theta, most_inner)  ##  1 x K\n",
    "                sum_z = sum(unit)\n",
    "                log_sum_z = np.log(sum_z)\n",
    "                each_d += log_sum_z\n",
    "            llk += each_d\n",
    "        return llk\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class document:\n",
    "    \n",
    "    def __init__(self, l, K):\n",
    "        line = l.rstrip().split('\\t')\n",
    "        self.corpus = int(line[0])\n",
    "        self.words = line[1:]\n",
    "        ### randomly assign z and x when creating the object\n",
    "        self.v = len(self.words)   # total number of words in document d\n",
    "        random.seed(0)\n",
    "        self.z = random.choice(range(K), size = self.v)   # z: 1 x v. the index of topics for all words.\n",
    "        self.x = random.choice([0,1], size = self.v)      # x: 1 x v. which phi the word is drawn from.  \n",
    "        self.ndk = np.zeros(K)\n",
    "        self.cal_ndk(K)\n",
    "        \n",
    "    def word_index(self, words_index):\n",
    "        for t in range(self.v):\n",
    "            self.words[t] = words_index[self.words[t]]\n",
    "                \n",
    "    def cal_nwk(self, V, K):\n",
    "        ### append assignments (z) to words. nwk: V x K. ordered lists of ordered lists\n",
    "        self.nwk, self.nwk_0, self.nwk_1 = np.zeros([V,K]), np.zeros([V,K]), np.zeros([V,K])\n",
    "        for t in range(self.v):\n",
    "            w_idx = self.words[t]tvgvg\n",
    "            w_z  = self.z[t]\n",
    "            w_x = self.x[t]\n",
    "            if w_x == 0:\n",
    "                self.nwk[w_idx, w_z] += 1 \n",
    "            elif w_x == 1 and self.corpus == 0:\n",
    "                self.nwk_0[w_idx, w_z] += 1\n",
    "            elif w_x == 1 and self.corpus == 1:\n",
    "                self.nwk_1[w_idx, w_z] += 1\n",
    "        \n",
    "    def cal_ndk(self, K):\n",
    "        ### number of words assigned in each topic\n",
    "        ndk = Counter(self.z)\n",
    "        for k in range(K):\n",
    "            self.ndk[k] = ndk[k]      ## 1 x K\n",
    "\n",
    "    def update_z_x_train(self, Nk, Nk_0, Nk_1, Nwk, Nwk_0, Nwk_1, V, K):\n",
    "        ### update z and x word by word      \n",
    "        Nd = self.v - 1 \n",
    "        for t in range(self.v):\n",
    "            current_k = self.z[t]                     # current topic / k\n",
    "            current_v = self.words[t]                 # current word location\n",
    "            current_x = self.x[t]                     # current x\n",
    "            ### update z and x\n",
    "            ## for numerator\n",
    "            self.ndk[current_k] -= 1\n",
    "            ## first term: doens't change for x = 0/1\n",
    "            first_term = np.divide((self.ndk + alpha), (Nd + K * alpha))    ## 1 x K\n",
    "            ## second term\n",
    "            if self.corpus == 0:\n",
    "                temp_Nwk = Nwk_0\n",
    "                temp_Nk = Nk_0\n",
    "            elif self.corpus == 1:\n",
    "                temp_Nwk = Nwk_1\n",
    "                temp_Nk = Nk_1\n",
    "            # delete the current assignment\n",
    "            if current_x == 0:\n",
    "                Nk[current_k] -= 1\n",
    "                Nwk[current_v, current_k] -= 1\n",
    "            elif current_x == 1:\n",
    "                temp_Nk[current_k] -= 1\n",
    "                temp_Nwk[current_v, current_k] -= 1\n",
    "            ## x = 0\n",
    "            second_term = np.divide((Nwk[current_v,] + beta), (Nk + V * beta))   ## 1 x K\n",
    "            prop_0 = (1-lmda) * np.multiply(first_term, second_term)   ## 1 x K\n",
    "            ## x = 1\n",
    "            second_term = np.divide((temp_Nwk[current_v,] + beta), (temp_Nk + V * beta))   ## 1 x K\n",
    "            prop_1 = lmda * np.multiply(first_term, second_term)   ## 1 x K\n",
    "            # add back the deletion\n",
    "            if current_x == 0:\n",
    "                Nk[current_k] += 1\n",
    "                Nwk[current_v, current_k] += 1\n",
    "            elif current_x == 1:\n",
    "                temp_Nk[current_k] += 1\n",
    "                temp_Nwk[current_v, current_k] += 1\n",
    "            ## sample x, z\n",
    "            pdf = list(prop_0) + list(prop_1)\n",
    "            pdf = pdf / np.sum(pdf) \n",
    "            if (pdf<0).any():\n",
    "                print current_v\n",
    "            assert (pdf>=0).all()\n",
    "            random_zx = np.random.random_sample()\n",
    "            idx = bisect.bisect_left(np.cumsum(pdf), random_zx)\n",
    "            self.z[t] = idx % K\n",
    "            self.x[t] = idx / K\n",
    "            ## update Nwk and Nwk_c, and ndk\n",
    "            if current_x == 0 and self.x[t] == 1:\n",
    "                Nk[current_k] -= 1\n",
    "                Nwk[current_v, current_k] -= 1\n",
    "                temp_Nwk[current_v, self.z[t]] += 1\n",
    "                temp_Nk[self.z[t]] += 1\n",
    "            elif current_x == 0 and self.x[t] == 0:\n",
    "                Nk[current_k] -= 1\n",
    "                Nwk[current_v, current_k] -= 1\n",
    "                Nwk[current_v, self.z[t]] += 1\n",
    "                Nk[self.z[t]] += 1\n",
    "            elif current_x == 1 and self.x[t] == 0:\n",
    "                temp_Nk[current_k] -= 1\n",
    "                temp_Nwk[current_v, current_k] -= 1\n",
    "                Nwk[current_v, self.z[t]] += 1 \n",
    "                Nk[self.z[t]] += 1 \n",
    "            elif current_x == 1 and self.x[t] == 1:\n",
    "                temp_Nk[current_k] -= 1\n",
    "                temp_Nwk[current_v, current_k] -= 1\n",
    "                temp_Nwk[current_v, self.z[t]] += 1\n",
    "                temp_Nk[self.z[t]] += 1\n",
    "            self.ndk[self.z[t]] += 1\n",
    "\n",
    "    def update_z_x_test(self, Nk, Nk_0, Nk_1, Nwk, Nwk_0, Nwk_1, V, K):\n",
    "        ### update z and x word by word      \n",
    "        Nd = self.v - 1 \n",
    "        for t in range(self.v):\n",
    "            current_k = self.z[t]                     # current topic / k\n",
    "            current_v = self.words[t]                 # current word location\n",
    "            current_x = self.x[t]                     # current x\n",
    "            ### update z and x\n",
    "            ## for numerator\n",
    "            self.ndk[current_k] -= 1\n",
    "            ## first term: doesn't change for x = 0 and x = 1\n",
    "            first_term = np.divide((self.ndk + alpha), (Nd + K * alpha))    ## 1 x K\n",
    "            ## second term\n",
    "            # x = 0\n",
    "            second_term = np.divide((Nwk[current_v,] + beta), (Nk + V * beta))   ## 1 x K\n",
    "            prop_0 = (1-lmda) * np.multiply(first_term, second_term)   ## 1 x K\n",
    "            ## x = 1\n",
    "            # choose c\n",
    "            if self.corpus == 0:\n",
    "                temp_Nwk = Nwk_0\n",
    "                temp_Nk = Nk_0\n",
    "            elif self.corpus == 1:\n",
    "                temp_Nwk = Nwk_1\n",
    "                temp_Nk = Nk_1\n",
    "            ## calcualte\n",
    "            second_term = np.divide((temp_Nwk[current_v,] + beta), (temp_Nk + V * beta))   ## 1 x K\n",
    "            prop_1 = lmda * np.multiply(first_term, second_term)   ## 1 x K\n",
    "            ## sample x, z\n",
    "            pdf = list(prop_0) + list(prop_1)\n",
    "            pdf = pdf / np.sum(pdf)\n",
    "            if (pdf<0).any():\n",
    "                print current_v, prop_0, prop_1, self.ndk, Nwk[current_v,], temp_Nwk[current_v,]\n",
    "            assert (pdf>=0).all()\n",
    "            random_zx = np.random.random_sample()\n",
    "            idx = bisect.bisect_left(np.cumsum(pdf), random_zx)\n",
    "            self.z[t] = idx % K\n",
    "            self.x[t] = idx / K\n",
    "            ### update ndk\n",
    "            self.ndk[self.z[t]] += 1\n",
    "    \n",
    "    def estimate_theta(self, K):\n",
    "        ### equation 5\n",
    "        self.theta = np.divide((self.ndk + alpha), (self.v + K * alpha))    ### D x K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def phi_addindex(phi, words_index):\n",
    "    keys = words_index.keys()\n",
    "    idx = words_index.values()\n",
    "    words = []\n",
    "    for t in range(len(phi)):\n",
    "        words.append(keys[idx.index(t)])\n",
    "    phi_df = pd.DataFrame(phi)\n",
    "    phi_df['words'] = words\n",
    "    df = phi_df.set_index(['words'])\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(train='gibbs/input-train.txt', test='gibbs/input-test.txt', output='gibbs/output.txt', K=10, lmda=.5, alpha=.1, beta=.01, iter_max=200, burn_in=100):\n",
    "    filename_train = os.path.join(train)\n",
    "    filename_test = os.path.join(test)\n",
    "    data = literature(filename_train, filename_test, K)\n",
    "\n",
    "    theta = np.zeros([len(data.documents_train), data.K])\n",
    "    phi, phi_0, phi_1 = np.zeros([data.V, data.K]), np.zeros([data.V, data.K]), np.zeros([data.V, data.K])\n",
    "    llk_train, llk_test = [], []\n",
    "    t = []\n",
    "\n",
    "    for it in range(iter_max):\n",
    "        START = time.time()\n",
    "        ### train\n",
    "        data.gibbs_sampler('train')            ## (a)\n",
    "        data.MAP_estimate('train')             ## (b)\n",
    "        ### test\n",
    "        data.gibbs_sampler('test')\n",
    "        data.MAP_estimate('test')\n",
    "        if it > burn_in:                       ## (c)\n",
    "            phi += data.phi\n",
    "            phi_0 += data.phi_0\n",
    "            phi_1 += data.phi_1\n",
    "            for d in xrange(len(data.documents_train)):\n",
    "                theta[d] = data.documents_train[d].theta\n",
    "        ### llk\n",
    "        llk_train.append(data.cal_llk('train'))  \n",
    "        llk_test.append(data.cal_llk('test')) \n",
    "        t.append((time.time() - START))\n",
    "\n",
    "    pd.DataFrame(llk_train).to_csv('%s-trainll' % output, sep=' ', index=False , header=False)\n",
    "    pd.DataFrame(llk_test).to_csv('%s-testll' % output, sep=' ', index=False, header=False)\n",
    "    pd.DataFrame(t).to_csv('%s-t' % output, sep=' ', index=False, header=False)\n",
    "    pd.DataFrame(theta / (iter_max - burn_in)).to_csv('%s-theta' % output, sep=' ', index=False, header=False)\n",
    "    phi = phi_addindex(phi / (iter_max - burn_in), data.words_index)\n",
    "    phi.to_csv('%s-phi' % output, sep=' ', index=True, header=False)\n",
    "    phi0 = phi_addindex(phi_0 / (iter_max - burn_in), data.words_index)\n",
    "    phi0.to_csv('%s-phi0' % output, sep=' ', index=True, header=False)\n",
    "    phi1 = phi_addindex(phi_1 / (iter_max - burn_in), data.words_index)\n",
    "    phi1.to_csv('%s-phi1' % output, sep=' ', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmda=.5\n",
    "alpha=.1\n",
    "beta=.01\n",
    "\n",
    "main(lmda=lmda, alpha=alpha, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
