{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Use fixed empirical covariance matrix for all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.special import digamma\n",
    "from scipy.stats import gamma\n",
    "from scipy.special import gammaln\n",
    "from scipy.special import multigammaln\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import time\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "import dill\n",
    "\n",
    "## for simulation\n",
    "from numpy.random import dirichlet\n",
    "from numpy.random import multinomial\n",
    "from numpy.random import multivariate_normal\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name wishart",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-496c59e61bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultinomial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwishart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name wishart"
     ]
    }
   ],
   "source": [
    "### Run on HHPC\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import dill\n",
    "\n",
    "## for simulation\n",
    "from numpy.random import dirichlet\n",
    "from numpy.random import multinomial\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.stats import wishart\n",
    "\n",
    "\n",
    "\n",
    "class LDA_GMM:\n",
    "\n",
    "    def __init__(self, alphai, M, K, V, W0_scale):\n",
    "\n",
    "        ### W0_scale controls the covariance matrix for the modules\n",
    "        ### Since wishart cannot be run on local machine, need to generate the Lambdas from HHPC\n",
    "        ### W0_scale and fn should be corresponding\n",
    "\n",
    "\n",
    "        ### What needs to be stored: \n",
    "        #   1) the observed variables -- for inference: M, Nd, K, V, Wn\n",
    "        #   2) true modules -- for evaluation: word_assginment_z, theta\n",
    "        #   3) superparameters -- for better inference: alphai, nu0, W0, m0, beta0\n",
    "\n",
    "        self.alphai = alphai\n",
    "        self.M = M\n",
    "        self.K = K\n",
    "        self.V = V\n",
    "        np.random.seed(2)\n",
    "        self.Nd = np.random.choice(range(150,300),M)   ## number of regions in each gene\n",
    "\n",
    "        ## cluster assignment for each region\n",
    "        self.theta = dirichlet(np.ones(K)*alphai, M)\n",
    "        word_assginment_z = []\n",
    "        for d in xrange(M):\n",
    "            word_assginment_z.append([multinomial(1, self.theta[d]) for t in xrange(self.Nd[d])])\n",
    "            word_assginment_z[d] = np.array(word_assginment_z[d])\n",
    "\n",
    "        ## modules\n",
    "        self.nu0 = V + 5.0\n",
    "        self.m0 = np.zeros(V)\n",
    "        self.beta0 = 1.0\n",
    "        self.W0 = np.identity(V) * W0_scale\n",
    "\n",
    "        Lambbda = wishart.rvs(df=self.nu0, scale=self.W0, size=K, random_state=0)\n",
    "        Sigma = [np.linalg.inv(lll) for lll in Lambbda]\n",
    "        mu = [multivariate_normal(self.m0, Sigma[k]/self.beta0, size=1)[0] for k in xrange(K)]\n",
    "\n",
    "        ## observations\n",
    "        words_values = []\n",
    "        for d in xrange(M):\n",
    "            word_assginment_z_for_this_d = [np.where(zzz)[0][0] for zzz in word_assginment_z[d]]\n",
    "            words_values.append([multivariate_normal(mu[zzz], Sigma[zzz], size = 1)[0] for zzz in word_assginment_z_for_this_d])\n",
    "            words_values[d] = np.array(words_values[d])\n",
    "        self.Wn = words_values\n",
    "        self.word_assginment_z = word_assginment_z\n",
    "\n",
    "        print np.std(np.concatenate(words_values))\n",
    "        print np.linalg.det(np.cov(np.transpose(np.concatenate(words_values))))\n",
    "        print np.linalg.cond(np.cov(np.transpose(np.concatenate(words_values))))\n",
    "\n",
    "if 0:\n",
    "    W0_scale = 0.10\n",
    "    OBJECT = LDA_GMM(0.25, 10, 4, 150, W0_scale)\n",
    "    pickle.dump(OBJECT, open('W0_scale_%s.p' % str(W0_scale), \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class LDA_GMM:\n",
    "\n",
    "    def __init__(self, fn):\n",
    "        \n",
    "        ### Generate in HHPC, and saved using pickle\n",
    "                \n",
    "        ### What needs to be stored: \n",
    "        #   1) the observed variables -- for inference: M, Nd, K, V, Wn\n",
    "        #   2) true modules -- for evaluation: word_assginment_z, theta\n",
    "        #   3) superparameters -- for better inference: alphai, nu0, W0, m0, beta0\n",
    "\n",
    "\n",
    "        downloaded_Data = dill.load( open( \"VI/%s.p\" % fn, \"rb\" ) )\n",
    "        self.M = downloaded_Data.M\n",
    "        self.Nd = downloaded_Data.Nd\n",
    "        self.K = downloaded_Data.K\n",
    "        self.V = downloaded_Data.V\n",
    "        self.Wn = downloaded_Data.Wn\n",
    "        \n",
    "        self.word_assginment_z = downloaded_Data.word_assginment_z\n",
    "        self.theta = downloaded_Data.theta\n",
    "        \n",
    "        self.alphai = downloaded_Data.alphai\n",
    "        self.nu0 = downloaded_Data.nu0\n",
    "        self.W0 = downloaded_Data.W0\n",
    "        self.m0 = downloaded_Data.m0\n",
    "        self.beta0 = downloaded_Data.beta0\n",
    "        \n",
    "        \n",
    "        Y = np.cov(np.transpose(np.concatenate(self.Wn)))\n",
    "        print 'Determint of the empricial covariance matrix is: ', np.linalg.det(Y)\n",
    "        self.Wk = np.linalg.inv(Y)\n",
    "        \n",
    "    def initialize(self):        \n",
    "        \n",
    "        self.W0_inv = np.linalg.inv(self.W0)\n",
    "        \n",
    "        #### randomly initialize phi \n",
    "        self.phi = []\n",
    "        for d in xrange(self.M):\n",
    "            self.phi.append(vector_sum_to_1(self.Nd[d], self.K))\n",
    "            self.phi[d] = np.array(map(lambda x: (x+1e-50) / np.sum(x+1e-50), self.phi[d]))\n",
    "            \n",
    "        ## Define some statistics for convinence\n",
    "        # Nk\n",
    "        self.Nk = np.sum([np.sum(self.phi[d], axis=0) for d in xrange(self.M)], axis=0)\n",
    "        # weighted mean value in each module\n",
    "        self.wk_bar = np.array([sum(l) for l in zip(*[np.dot(np.transpose(self.phi[t]), self.Wn[t]) for t in xrange(self.M)])])\n",
    "        self.wk_bar = np.array([self.wk_bar[k,:] / self.Nk[k] for k in xrange(self.K)])\n",
    "        # weighted covariance\n",
    "        self.Sk = []\n",
    "        for k in xrange(self.K):\n",
    "            centered_Wnd = np.concatenate(self.Wn) - self.wk_bar[k,:][np.newaxis]\n",
    "            self.Sk.append(np.dot(np.concatenate(self.phi)[:,k][np.newaxis] * np.transpose(centered_Wnd), centered_Wnd) / self.Nk[k])\n",
    "\n",
    "            \n",
    "        #### Initialization the other parameters\n",
    "        # Lambda\n",
    "        self.betak = self.beta0 + self.Nk\n",
    "        self.nuk   = self.nu0 + self.Nk\n",
    "        self.mk    = [(self.beta0 * self.m0 + self.Nk[k] * self.wk_bar[k,:]) / (self.beta0 + self.Nk[k]) for k in xrange(self.K)]\n",
    "        \n",
    "        self.Eq_lnDetLambda = np.zeros(self.K)\n",
    "        for k in xrange(self.K):\n",
    "            self.Eq_lnDetLambda[k] = np.sum([digamma((self.nuk[k] + 1 - j)/2) for j in xrange(self.V)]) + \\\n",
    "                                     self.V * np.log(2) + np.linalg.slogdet(self.Wk)[1]        \n",
    "        \n",
    "        # gammma\n",
    "        self.gammma = [np.sum(self.phi[d], axis=0) + self.alphai for d in xrange(self.M)]\n",
    "        self.gammma = np.array(self.gammma)\n",
    "                \n",
    "        #### storage\n",
    "        self.ELBO = []\n",
    "        self.T = []\n",
    "        self.updated = []        \n",
    "        self.compute_ELBO('Initial')\n",
    "        self.old_ELBO = self.ELBO[-1].copy()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def update(self, max_iter):\n",
    "        \n",
    "        episron = 1e-3\n",
    "        iterations = 1\n",
    "\n",
    "        while iterations < max_iter: \n",
    "            \n",
    "            START = time.time()\n",
    "            \n",
    "            ### update phi\n",
    "            \n",
    "            S = time.time()\n",
    "            for d in xrange(self.M):\n",
    "                \n",
    "                ## [Nd, K], every row is the same for each matrix.\n",
    "                Eq_mu_Lambda_mean = []\n",
    "                for k in xrange(self.K):\n",
    "                    wk_minus_mk = self.Wn[d] - self.mk[k][np.newaxis]\n",
    "                    Eq_mu_Lambda_mean.append(np.diag(self.V / self.betak[k] + \\\n",
    "                                              self.nuk[k] * np.dot(np.dot(wk_minus_mk, self.Wk), np.transpose(wk_minus_mk))))\n",
    "                Eq_mu_Lambda_mean = np.transpose(Eq_mu_Lambda_mean)\n",
    "                self.Eq_mu_Lambda_mean = Eq_mu_Lambda_mean\n",
    "                \n",
    "                self.phi[d] = np.exp(digamma(self.gammma[d,:])[np.newaxis]  - Eq_mu_Lambda_mean/2)\n",
    "                self.phi[d] = np.array(map(lambda x: (x+1e-50) / np.sum(x+1e-50), self.phi[d]))\n",
    "                 \n",
    "            ### Define some statistics for convinence\n",
    "            # Nk\n",
    "            self.Nk = np.sum([np.sum(self.phi[d], axis=0) for d in xrange(self.M)], axis=0)\n",
    "            # weighted mean value in each module\n",
    "            self.wk_bar = np.array([sum(l) for l in zip(*[np.dot(np.transpose(self.phi[t]), self.Wn[t]) for t in xrange(self.M)])])\n",
    "            self.wk_bar = np.array([self.wk_bar[k,:] / self.Nk[k] for k in xrange(self.K)])\n",
    "            # weighted covariance\n",
    "            self.Sk = []\n",
    "            for k in xrange(self.K):\n",
    "                centered_Wnd = np.concatenate(self.Wn) - self.wk_bar[k,:][np.newaxis]\n",
    "                self.Sk.append(np.dot(np.concatenate(self.phi)[:,k][np.newaxis] * np.transpose(centered_Wnd), centered_Wnd) / self.Nk[k])\n",
    "            #print np.mean([np.linalg.det(x) for x in self.Sk])\n",
    "                \n",
    "            print 'Update phis:', time.time() - S\n",
    "            ### compute ELBO\n",
    "            self.compute_ELBO('phi')\n",
    "                \n",
    "\n",
    "            \n",
    "            ### update gammma\n",
    "            self.gammma = [np.sum(self.phi[d], axis=0) + self.alphai for d in xrange(self.M)]\n",
    "            self.gammma = np.array(self.gammma)\n",
    "            self.compute_ELBO('gamma')\n",
    "            \n",
    "            S = time.time()\n",
    "            \n",
    "            ### update lambda\n",
    "            self.betak = self.beta0 + self.Nk\n",
    "            self.mk = [(self.beta0 * self.m0 + self.Nk[k] * self.wk_bar[k,:]) / (self.beta0 + self.Nk[k]) for k in xrange(self.K)]\n",
    "            self.nuk = self.nu0 + self.Nk           \n",
    "            for k in xrange(self.K):\n",
    "                self.Eq_lnDetLambda[k] = np.sum([digamma((self.nuk[k] + 1 - j)/2) for j in xrange(self.V)]) + \\\n",
    "                                         self.V * np.log(2) + np.linalg.slogdet(self.Wk)[1]\n",
    "            print 'update Lambdas:', time.time()-S\n",
    "                    \n",
    "            self.compute_ELBO('Lambdas')\n",
    "            \n",
    "            assert ~np.isnan(sum([np.sum(self.phi[t]) for t in xrange(self.M)]))\n",
    "            assert ~np.isnan(np.sum(self.gammma))\n",
    "            assert ~np.isnan(np.sum(self.Wk))\n",
    "        \n",
    "            #print Counter([a for b in [np.argmax(self.phi[d], axis=1) for d in xrange(self.M)] for a in b])\n",
    "    \n",
    "            if np.abs(self.ELBO[-1] - self.old_ELBO) < episron:\n",
    "                print 'Converged after %d iterations\\n' % iterations\n",
    "                break\n",
    "            else:\n",
    "                iterations += 1\n",
    "                self.old_ELBO = self.ELBO[-1].copy()\n",
    "                self.T.append(time.time() - START)                \n",
    "\n",
    "                \n",
    "    def compute_ELBO(self, qi_updated):\n",
    "                        \n",
    "        gammma_term = np.sum(gammaln(self.gammma))\n",
    "        phi_term = -np.sum([np.sum(t * np.log(t)) for t in self.phi])\n",
    "\n",
    "        mu_lambda_term = []\n",
    "        for k in xrange(self.K):\n",
    "            temp1 = (self.Nk[k] + self.nu0 - self.V - 2) * self.Eq_lnDetLambda[k]\n",
    "            temp2 = self.V * (self.Nk[k] + self.beta0) / self.betak[k]\n",
    "            temp4 = self.nuk[k] * (self.Nk[k]  *  np.dot(np.dot(self.wk_bar[k] - self.mk[k], self.Wk)[np.newaxis], \\\n",
    "                                                         (self.wk_bar[k] - self.mk[k])[np.newaxis].T) + \\\n",
    "                                        self.beta0 * np.dot(np.dot(self.mk[k] - self.m0, self.Wk)[np.newaxis], \\\n",
    "                                                        (self.mk[k] - self.m0)[np.newaxis].T))\n",
    "            temp4 = temp4[0,0]\n",
    "            temp5 = self.nuk[k] * (self.Nk[k] * np.matrix.trace(np.dot(self.Sk[k], self.Wk)) + \\\n",
    "                                    np.matrix.trace(np.dot(np.linalg.inv(self.W0), self.Wk)))\n",
    "            temp6 = self.V * self.Nk[k] * np.log(2*np.math.pi) + self.V * np.log(self.betak[k]/(2*np.math.pi)) \n",
    "            H_q_Lambdak = (self.V+1)/2 * np.linalg.slogdet(self.Wk)[1] + \\\n",
    "                            multigammaln(self.nuk[k]/2, self.V) - \\\n",
    "                            (self.nuk[k]-self.V-1)/2 * np.sum([digamma((self.nuk[k]-j+1)/2) for j in xrange(self.V)]) + \\\n",
    "                            self.nuk[k] * self.V / 2\n",
    "            mu_lambda_term.append(temp1 - temp2 - temp4 - temp5 - temp6 + 2*H_q_Lambdak)\n",
    "            \n",
    "        ELBO = gammma_term + phi_term + np.sum(mu_lambda_term) / 2\n",
    "        self.ELBO.append(ELBO)\n",
    "        self.updated.append(qi_updated)\n",
    "\n",
    "        #print qi_updated, ELBO\n",
    "        \n",
    "        \n",
    "        \n",
    "def vector_sum_to_1(rowN, colN):\n",
    "    ## row sum up to 1\n",
    "    np.random.seed(10)\n",
    "    vc = np.random.sample([rowN, colN])\n",
    "    vc = map(lambda x: x / np.sum(x), vc)\n",
    "    return np.array(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determint of the empricial covariance matrix is:  1.09037423854e-41\n",
      "Update phis: 8.29326200485\n",
      "update Lambdas: 0.00946688652039\n",
      "Update phis: 8.31288313866\n",
      "update Lambdas: 0.00825500488281\n",
      "Converged after 2 iterations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ZZZ = LDA_GMM('simulated_data/M_100_K_4_V_150_W0_scale_0.1')\n",
    "ZZZ.initialize()\n",
    "iterations = 10\n",
    "ZZZ.update(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 20, V = 124, K = 4\n",
      "2.95470486568e+45\n",
      "1245.61064699\n",
      "-971455.48716\n",
      "-971530.105155\n",
      "-971539.987434\n",
      "-971526.346528\n",
      "-971541.9162\n",
      "-971647.406103\n",
      "-971644.151413\n",
      "-971702.564773\n",
      "-971696.832893\n",
      "-971698.606704\n",
      "-971681.243912\n",
      "-971763.172661\n",
      "-971868.848403\n",
      "-971936.650516\n",
      "-971930.816248\n",
      "-971967.730835\n",
      "-971968.3694\n",
      "-971985.492096\n",
      "-971945.574825\n",
      "-971934.714481\n",
      "-971930.583459\n",
      "-971907.215745\n",
      "-971907.071232\n",
      "-971845.326482\n",
      "-969485.121714\n",
      "-969380.553014\n",
      "-969207.281642\n",
      "-968795.91498\n",
      "-968553.028671\n",
      "-968459.166151\n",
      "-968078.90763\n",
      "-967996.337876\n",
      "-967573.388345\n",
      "-967005.545309\n",
      "-966716.899704\n",
      "-966664.998378\n",
      "-966567.480872\n",
      "-966496.891466\n",
      "-966236.15521\n",
      "-966095.640124\n",
      "-965870.393245\n",
      "-965662.707278\n",
      "-965271.181298\n",
      "-964929.736828\n",
      "-964728.8424\n",
      "-964254.836817\n",
      "-964251.827767\n",
      "-963717.581272\n",
      "-948732.194457\n",
      "-948145.232963\n",
      "-946744.362743\n",
      "-943985.205374\n",
      "-942384.558881\n",
      "-941631.193285\n",
      "-938900.603711\n",
      "-937669.687211\n",
      "-934843.809162\n",
      "-931425.406746\n",
      "-929453.494653\n",
      "-928885.780713\n",
      "-928267.74949\n",
      "-927068.650084\n",
      "-925522.573768\n",
      "-924846.43451\n",
      "-923235.944539\n",
      "-921826.272135\n",
      "-919910.509083\n",
      "-918052.385221\n",
      "-916596.425404\n",
      "-915447.484828\n",
      "-915438.90441\n",
      "-914625.021798\n",
      "-848761.227887\n",
      "-847518.833382\n",
      "-845755.061305\n",
      "-845316.476023\n",
      "-842639.024318\n",
      "-840047.240475\n",
      "-839437.925851\n",
      "-834427.188414\n",
      "-834117.055812\n",
      "-833251.998361\n",
      "-832575.770697\n",
      "-830492.244899\n",
      "-830162.018033\n",
      "-826257.078782\n",
      "-824342.025309\n",
      "-823641.620016\n",
      "-823226.122665\n",
      "-821717.461678\n",
      "-820649.279052\n",
      "-818804.461279\n",
      "-817553.735306\n",
      "-816861.705101\n",
      "-816842.010801\n",
      "-816670.110838\n",
      "-780258.090151\n",
      "-778932.175182\n",
      "-778853.677056\n",
      "-778853.509649\n",
      "-778819.442298\n",
      "-777788.257229\n",
      "-777666.470999\n",
      "-777651.823522\n",
      "-777651.825611\n",
      "-777563.682885\n",
      "-777500.208405\n",
      "-777026.601589\n",
      "-775646.85231\n",
      "-775526.327851\n",
      "-775146.266609\n",
      "-774547.110793\n",
      "-774313.083568\n",
      "-774201.747016\n",
      "-774049.69119\n",
      "-773905.821266\n",
      "-773684.421542\n",
      "-773591.734191\n",
      "-773590.182188\n",
      "-773430.082677\n",
      "-765013.686951\n",
      "-761785.692454\n",
      "-761734.755831\n",
      "-761711.539321\n",
      "-761580.803754\n",
      "-758980.918339\n",
      "-758280.428231\n",
      "-758280.428225\n",
      "-758216.490597\n",
      "-758020.091529\n",
      "-757927.066275\n",
      "-755403.072729\n",
      "-751677.161797\n",
      "-751048.247309\n",
      "-750248.282342\n",
      "-748286.068704\n",
      "-748131.605466\n",
      "-747657.35292\n",
      "-747420.899058\n",
      "-746987.606313\n",
      "-746636.230709\n",
      "-746306.92013\n",
      "-746293.231203\n",
      "-745899.791751\n",
      "-703285.288992\n",
      "-699183.142322\n",
      "-698931.223452\n",
      "-698931.223452\n",
      "-698931.22344\n",
      "-696203.798292\n",
      "-695610.214353\n",
      "-695610.214353\n",
      "-695608.627806\n",
      "-695283.451302\n",
      "-695283.451276\n",
      "-692042.516284\n",
      "-687909.86559\n",
      "-687501.107038\n",
      "-687315.161983\n",
      "-684552.746245\n",
      "-684225.607634\n",
      "-682858.951275\n",
      "-682831.139801\n",
      "-682755.284006\n",
      "-682600.012934\n",
      "-682356.883105\n",
      "-682354.102554\n",
      "-682229.787925\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.297301\n",
      "-652612.2973\n",
      "-652612.286221\n",
      "-652612.286221\n",
      "-652612.286221\n",
      "-652612.28199\n",
      "-652612.28199\n",
      "-652612.28199\n",
      "-652612.28199\n",
      "-652612.28199\n",
      "-652612.28199\n",
      "-652612.281971\n",
      "-652612.281971\n",
      "-652612.281971\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "-652612.281998\n",
      "Converged after 9 iterations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn = \"Simulated_Lambbda_median_2_W0_median_2_data_std\"\n",
    "\n",
    "Z = LDA_GMM('%s.npz' % fn, 4)\n",
    "Z.initialize()\n",
    "\n",
    "\n",
    "iterations = 10\n",
    "Z.update(iterations)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(Z.ELBO)), Z.ELBO)\n",
    "plt.savefig('VI/%s_ELBO.png' % fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.60339802678e-13\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "Z = ZZZ\n",
    "\n",
    "allsamples = np.array(np.concatenate(Z.Wn))\n",
    "GMM_model = GaussianMixture(n_components=Z.K)\n",
    "GMM_model.fit(allsamples)\n",
    "GMM_pred = GMM_model.predict_proba(allsamples)\n",
    "\n",
    "GMM_assignment = [np.argmax(x) for x in GMM_pred]\n",
    "resulting_assignment = [np.argmax(x) for x in np.concatenate(Z.phi)]\n",
    "true_assignment = [np.argmax(x) for x in np.concatenate(Z.word_assginment_z)]\n",
    "print adjusted_rand_score(resulting_assignment, true_assignment)\n",
    "print adjusted_rand_score(GMM_assignment, true_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685993664784\n"
     ]
    }
   ],
   "source": [
    "gene_assignment = [np.argmax(x) for x in Z.gammma]\n",
    "true_gene_assignment = [np.argmax(x) for x in Z.theta]\n",
    "print adjusted_rand_score(gene_assignment, true_gene_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-434-e465dc2b25bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mttt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mphiiii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrease_ELBO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mttt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mttt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgammmaaaa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrease_ELBO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mttt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mbetakkkk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrease_ELBO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mttt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmkkkkk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrease_ELBO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mttt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "phiiii = []\n",
    "gammmaaaa = [] \n",
    "betakkkk = []\n",
    "mkkkkk = []\n",
    "Wkkkkk = []\n",
    "nukkkkk = []\n",
    "a = Z.M + 8\n",
    "b = Z.M\n",
    "increase_ELBO = [j-i for i, j in zip(Z.ELBO[:-1], Z.ELBO[1:])]\n",
    "for ttt in xrange(8-1):\n",
    "    phiiii.append(increase_ELBO[ttt*a:ttt*a+b])\n",
    "    gammmaaaa.append(increase_ELBO[ttt*a+b])\n",
    "    betakkkk.append(increase_ELBO[ttt*a+b+1])\n",
    "    mkkkkk.append(increase_ELBO[ttt*a+b+2])\n",
    "    Wkkkkk.append(increase_ELBO[ttt*a+b+3:ttt*a+b+7])\n",
    "    nukkkkk.append(increase_ELBO[ttt*a+b+7])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
